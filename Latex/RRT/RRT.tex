\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage{fourier}

\usepackage[english]{babel}															% English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[pdftex]{graphicx}	
\usepackage{url}
\usepackage{hyperref}

\usepackage{algorithm}
%\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}


%%% Custom sectioning
\usepackage{sectsty}
\allsectionsfont{\centering \normalfont\scshape}
\usepackage{subfigure}
\usepackage{comment}


%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}											% No page header
\fancyfoot[L]{}											% Empty 
\fancyfoot[C]{}											% Empty
\fancyfoot[R]{\thepage}									% Pagenumbering
\renewcommand{\headrulewidth}{0pt}			% Remove header underlines
\renewcommand{\footrulewidth}{0pt}				% Remove footer underlines
\setlength{\headheight}{13.6pt}


%%% Equation and float numbering
%\numberwithin{equation}{section}		% Equationnumbering: section.eq#
%\numberwithin{figure}{section}			% Figurenumbering: section.fig#
%\numberwithin{table}{section}				% Tablenumbering: section.tab#


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule

\title{
		%\vspace{-1in} 	
		%\usefont{OT1}{bch}{b}{n}
		%\normalfont \normalsize \textsc{CS660 - Computer Networks} \\ [25pt]
		\normalfont
		\horrule{0.5pt} \\[0.4cm]
		\huge Multi-objective Rapidly Exploring Random Tree \\
		\horrule{2pt} \\[0.5cm]
}
\author{
		\normalfont %\normalsize
        Daqing Yi \\
}
%\date{}


%%% Begin document
\begin{document}
\maketitle

\section{Single-objective optimization}

\begin{algorithm}
\begin{algorithmic}[1]
\State $ V \leftarrow \{ x_{init} \} $; $ E \leftarrow \emptyset $; $ i \leftarrow 0 $
\While{ $ i < N $ }
\State $ G \leftarrow (V, E) $
\State $ x_{rand} \leftarrow $ \Call{ Sample }{$ i $} ; $ i \leftarrow i + 1 $
\State $ (V, E) \leftarrow $ \Call{ Extend }{$ G, x_{rand} $}
\EndWhile
\end{algorithmic}
\label{alg:rapidly_exploring_process}
\caption{Rapidly exploring process}
\end{algorithm}

\begin{itemize}
\item \textbf{Sampling} \\
$ \mbox{Sample(}) $ returns independent identically distributed samples from $ X_{\mbox{free}} $.
\item \textbf{Steering} \\
Given two points $ x $ and $ y $, it returns a point $ z $ that its distance to $ x $ is less than $ \eta $ and minimizes the distance to $ y $. 
$ \mbox{Steer}(x,y) = \arg \min_{ z \in \mathbb{R}^{d}, \lVert z -x \rVert \leq \eta } \lVert z - y \rVert $.
\item \textbf{Nearest neighbor} \\
It returns a vertex that is closest to point $ x $.
$ \mbox{Nearest}(G = (V,E), x) = \arg \min_{v \in V} \lVert x - v \rVert $.
\item \textbf{Near vertices} \\
$ \mbox{Near}(G, x, \eta) $ returns a set of all vertices within the closed ball of radius $ r_{n} $ centered at $ x $, in which $ r_{n} = \min \{ ( \frac{\gamma}{\xi_{d}} \frac{\log n}{n} )^{1/d}  , \eta \} $.
The volume of the ball is $ \min \{ \gamma \frac{\log n}{n} , \xi_{d} \eta^{d} \} $.
\item \textbf{Collision test} \\
$ \mbox{ObstacleFree}(x, x') $ returns True if $ [ x, x' ] \subset X_{ \mbox{free} } $, which means the line segment between $ x $ and $ x' $ lies in $ X _{ \mbox{free} } $.
\item \textbf{Line} \\
$ \mbox{Line}(x, x') : [0, s] \leftarrow X_{ \mbox{free} } $ denotes the path defined by $ \forall \tau \in [0, s], \sigma( \tau ) = \tau x + (s - \tau) x', s = \lVert x' -x \rVert $.
\item \textbf{Cost} \\
$ \mbox{Cost}( v ) $ is defined by the cost of the unique path from $ x_{ \mbox{init} } $ to a vertex $ v \in V $.
$ \mbox{Cost}( x_{ \mbox{init} } ) = 0 $.
\end{itemize}

\begin{algorithm}
\begin{algorithmic}[1]
\State $ V' \leftarrow V $; $ E' \leftarrow E $
\State $ x_{nearest} \leftarrow $ \Call{Nearest}{$ G, x $}
\State $ x_{new} \leftarrow $ \Call{Steer}{$ x_{nearest}, x $}
\If{ \Call{ObstacleFree}{$ x_{nearest}, x_{new} $} }
\State $ V' \leftarrow V' \cup \{ x_{new} \} $
\State $ E' \leftarrow E' \cup \{ ( x_{nearest} , x_{new} ) \} $
\EndIf
\Return $ G' = (V', E') $ 
\end{algorithmic}
\label{alg:rrt_extend}
\caption{ $ \mbox{Extend}_{RRT} (G, x) $}
\end{algorithm}

\begin{algorithm}
\begin{algorithmic}[1]
\State $ V' \leftarrow V $; $ E' \leftarrow E $
\State $ x_{nearest} \leftarrow $ \Call{Nearest}{$ G, x $}
\State $ x_{new} \leftarrow $ \Call{Steer}{$ x_{nearest}, x $}
\If{ \Call{ObstacleFree}{$ x_{nearest}, x_{new} $} }
\State $ V' \leftarrow V' \cup \{ x_{new} \} $
\State $ E' \leftarrow E' \cup \{ ( x_{nearest} , x_{new} ) , ( x_{new} , x_{nearest} ) \} $
\State $ X_{near} \leftarrow $ \Call{Near}{$ G, x_{new}, | V | $}
\For{\textbf{each} $ x_{near} \in X_{near} $ }
\If{ \Call{ObstacleFree}{$ x_{new} , x_{near} $} }
$ E' \leftarrow E' \cup \{ ( x_{near}, x_{new} ) , ( x_{new} , x_{near} ) \} $
\EndIf
\EndFor
\EndIf
\Return $ G' = (V', E') $ 
\end{algorithmic}
\label{alg:rrg_extend}
\caption{ $ \mbox{Extend}_{RRG} (G, x) $}
\end{algorithm}

\begin{algorithm}
\begin{algorithmic}[1]
\State $ V' \leftarrow V $; $ E' \leftarrow E $
\State $ x_{nearest} \leftarrow $ \Call{Nearest}{$ G, x $}
\State $ x_{new} \leftarrow $ \Call{Steer}{$ x_{nearest}, x $}
\If{ \Call{ObstacleFree}{$ x_{nearest}, x_{new} $} }
\State $ V' \leftarrow V' \cup \{ x_{new} \} $
\State $ x_{min} \leftarrow x_{nearest} $
\State $ X_{near} \leftarrow $ \Call{Near}{$ G, x_{new}, | V | $}
\For{\textbf{each} $ x_{near} \in X_{near} $ }
\If{ \Call{ObstacleFree}{$ x_{new} , x_{near} $} }
\State $ c' \leftarrow $ \Call{Cost}{$ x_{near} $} $ + c( $ \Call{Line}{$ x_{near}, x_{new} $} $ ) $ 
\If{ $ c' < $ \Call{Cost}{$ x_{new} $} }
\State $ x_{min} \leftarrow x_{near} $
\EndIf
\EndIf
\EndFor
\State $ E' \leftarrow E' \cup \{ ( x_{min}, x_{new} ) \} $
\For{\textbf{each} $ x_{near} \in X_{near} \setminus \{ x_{min} \} $ }
\If{\Call{ObstacleFree}{$ x_{new} , x_{near} $} and \Call{Cost}{$ x_{near} $} > \Call{Cost}{$ x_{new} $} + c(\Call{Line}{$ x_{new}, x_{near} $}) }
\State $ x_{parent} \leftarrow $ \Call{Parent}{$ x_{near} $}
\State $ E' \leftarrow E' \setminus \{ ( x_{parent}, x_{near} ) \} $
\State $ E' \leftarrow E' \cup \{ ( x_{new}, x_{near} ) \} $
\EndIf
\EndFor
\EndIf
\Return $ G' = (V', E') $ 
\end{algorithmic}
\label{alg:rrtstar_extend}
\caption{ $ \mbox{Extend}_{RRT^{*}} (G, x) $}
\end{algorithm}

\section{Multi-objective optimization}

In a multi-objective path planning, the target is finding a set of non-dominant paths, which is a multi-objective optimization problem.
The multi-objective optimization can be solved using a Tchebycheff method.
With a utopia objective vector, we can have weighted Tchebycheff metric.
The function to be minimized is  
\begin{equation}
\max_{k=1, \cdots , K} [ \omega_{k}  ( f_{k} (x) - z^{**}_{k} ) ],
\end{equation}
where $ \mathbf{\omega} \in W = \{ \mathbf{\omega} \in \mathbf{\mathbb{R}}^{K} \mid 0 < \omega_{i} < 1 , \sum^{K}_{k=1} \omega_{k} = 1 \} $ .
The weakly Pareto optimal solutions can be avoided by a \emph{lexicographic weighted Tchebycheff problem}.
\begin{equation}
\mbox{lex minimize} \max_{k=1, \cdots , K} [ \omega_{k}  ( f_{k} (x) - z^{**}_{k} ) ], \\
\mbox{subject to } x \in S.
\end{equation}

Given the problem, we can generate a set of weights $ W $ for a set of sub-problems.
Each $ \omega \in W $ represents a sub-problem.
Let the size of set $ W $ be $ N $, which means that there are $ N $ sub-problems.
We need to change the definition of Cost function.

\begin{comment}
How to maintain the update of reference paths
Especially, when the reference paths changed, how to update the Cost of each link?
\end{comment}

\begin{algorithm}
\begin{algorithmic}[1]
\State $ V' \leftarrow V $; $ E' \leftarrow E $
\State $ x_{nearest} \leftarrow $ \Call{Nearest}{$ G, x $}
\State $ x_{new} \leftarrow $ \Call{Steer}{$ x_{nearest}, x $}
\If{ \Call{ObstacleFree}{$ x_{nearest}, x_{new} $} }
\State $ V' \leftarrow V' \cup \{ x_{new} \} $
\State $ x_{min} \leftarrow x_{nearest} $
\State $ X_{near} \leftarrow $ \Call{Near}{$ G, x_{new}, | V | $}
\For{\textbf{each} $ x_{near} \in X_{near} $ }
\If{ \Call{ObstacleFree}{$ x_{new} , x_{near} $} }
\State $ c' \leftarrow $ \Call{Cost}{$ x_{near} $} $ + c( $ \Call{Line}{$ x_{near}, x_{new} $} $ ) $ 
\If{ $ c' < $ \Call{Cost}{$ x_{new} $} }
\State $ x_{min} \leftarrow x_{near} $
\EndIf
\EndIf
\EndFor
\State $ E' \leftarrow E' \cup \{ ( x_{min}, x_{new} ) \} $
\For{ \textbf{each} $ x_{near} \in X_{near} \setminus \{ x_{min} \} $ }
\If{ \Call{ObstacleFree}{$ x_{new} , x_{near} $} and \Call{Cost}{$ x_{near} $} > \Call{Cost}{$ x_{new} $} + c(\Call{Line}{$ x_{new}, x_{near} $}) }
\State $ x_{parent} \leftarrow $ \Call{Parent}{$ x_{near} $}
\State $ E' \leftarrow E' \setminus \{ ( x_{parent}, x_{near} ) \} $
\State $ E' \leftarrow E' \cup \{ ( x_{new}, x_{near} ) \} $
\EndIf
\EndFor
\EndIf
\Return $ G' = (V', E') $ 
\end{algorithmic}
\label{alg:mo_rrtstar_extend}
\caption{ $ \mbox{Extend}_{MORRT^{*}} (G, x) $}
\end{algorithm}

\section{Theoretic analysis}

\begin{itemize}
	\item \emph{Reference tree}
	\item \emph{Subproblem tree}
\end{itemize}
The \emph{reference trees} are mutually independent. 
The \emph{subproblem trees} are mutually independent,  but depend on the \emph{reference trees}.

When a new node is added to a reference tree, there exists no influence to the subproblem tree.
When the 

\begin{itemize}
	\item \textbf{Lemma} The reference trees converge to the optimal
	\item \textbf{Lemma} The subproblem trees convege to the optimal of the subproblem
\end{itemize}




\bibliography{reference}
\bibliographystyle{plain}

\end{document}