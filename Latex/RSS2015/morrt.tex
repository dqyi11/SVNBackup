\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}

\usepackage[pdftex]{graphicx}
\usepackage{comment}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{amsthm}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}

\pdfinfo{
   /Author (Homer Simpson)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}

\begin{document}

% paper title
\title{ MORRF$^{*}$ : Sampling-Based Multi-Objective Motion Planning }

% You will get a Paper-ID when submitting a pdf file to the conference system
\author{Author Names Omitted for Anonymous Review. Paper-ID [add your ID here]}

\maketitle

\begin{abstract}
The importance of multi-objective path planning emerges with the increase of the requirement complexity of the robot's tasks.
The nature of the path planning problem makes that a few of the multi-objective optimization methods could not be directly imported to find the Pareto optimal solutions.
Inspired by the RRT* and the decomposition-based multi-objective optimization, we proposed a MORRF*(Multi-objective Rapidly exploring Random Forest*) in this paper that could effectively and efficiently find the Pareto optimal set of solutions.
The RRF consists of two types of tree structures, the reference tree and the subproblem tree.
Each reference tree explores a single objective to support the estimation of an Utopia position in the fitness space.
Each subproblem tree explores to find a solution to the assigned subproblem.
The solutions from all the trees form a set of Pareto optimal solutions. 

Theoretical analysis has been shown to support the feasibility of this algorithm and the asymptotic optimality. 
Simulations have been take to show the effectiveness and the efficiency of the MORRF*.
\end{abstract}

\IEEEpeerreviewmaketitle

\begin{comment}
Final Paper Submission Deadline: January 22, 2015, 23:59 PST

Papers can be up to 8 pages + references. 
This means that if more than 8 pages are used, the 9th and subsequent pages should contain ONLY references. 
The length requirement will be strictly enforced.
\end{comment}

\begin{comment}


\end{comment}

\section{Introduction}
\label{sec:intro}

\begin{comment}
(1) Why the multi-objective path planning is needed 
(2) Why the multi-objective path planning is hard (compare with a common multi-objective optimization problem)
\end{comment}

In many real applications of the robots, the complexity of tasks inherently implies more than single objective to be reached.
A robot in a search task is usually expected to maximize the search coverage with a consideration of the energy efficiency so that the execution time could also be expanded~\cite{yi2014supporting}. 
If there exists risk in the working environment, the robot should also try to avoid dangerous regions.
The multiple objectives in a planning optimization are usually incomparable and conflicted.
When those objectives could not be converted into single measurable objective, the multi-objective optimization will be needed to find a Pareto set of solutions.
In optimal design, a human interactive process can help the decision maker finding the most preferred solution, especially when the decision maker's preference is hard to precisely describe.

Finding Pareto optimal solutions in the multi-objective optimal path planning is much more difficult.
The popular methods in multi-objective optimization cannot be directly applied to a path planning problem.
One way is coding a path into a fixed-length solution by direction~\cite{Ahmed2013} or way points~\cite{5160222}.
Evolutionary algorithms could then be imported to search the optimal solutions.
In order to have a better approximation, the number of the segments consisted of a path cannot be too small.
This will make the solutions in a rather high dimension solution space.
However, representing the path trajectory by fixed number of segments might lead to problems in complex environment.
With different shapes and sizes of obstacles, it is hard to estimate how many segments would be enough to represent all the possible paths.
Allowing different segment numbers of the paths could help but the solution format won't fit the requirement of most of the evolutionary algorithms.
Also, when modeling a path into a point in a solution space, the obstacles in the working space could make a few of infeasible regions, which influences the continuity and increases the hardness of the heuristic-based search~\cite{5160222}~\cite{4358754}.

RRT(Rapidly exploring Random Tree) is a popular algorithm in finding feasible solutions from a start position to a goal position, which supports well in the environments with complex obstacles. 
The tree structure also guarantees a great efficiency on path search.
RRT* was recently introduced and has been proven to effectively finding an optimal path given enough sampling time~\cite{Karaman:2011:SAO:2000201.2000209}~\cite{Karaman.Frazzoli:RSS10}.
Similar with that decomposing the multi-objective optimization problem into a few subproblems in \cite{4358754}, we propose the MORRF*(Multi-Objective Rapidly exploring Random Forest*) to find a set of Pareto optimal paths from the start position to the goal position.

In this paper, we reviews a few existing multi-objective path planning algorithms and the works that inspires our algorithm in Section \ref{sec:related_works}.
With the definition of the multi-objective path planning problem, MORRF* is introduced and explained in Section \ref{sec:morrt}.
We provide theoretic analysis to support MORRF* in Section \ref{sec:theoretic_analysis}.
Simulation results are given in Section \ref{sec:simulation} to illustrate the performance of the proposed algorithm.

\section{Related works}
\label{sec:related_works}

Finding a set of Pareto optimal paths replies on exploring the non-dominance of the paths.
The implicit comparative property prevents a few pruning techniques for efficiency and the algorithm complexity would be greatly extended.
By modeling the working space into the connectivity of a graph structure, a multi-objective A* search could be applied to find the solution set in \cite{Mandow:2005:NAM:1642293.1642328}.
In a grid map, the paths could be coded into a sequence of directions from one cell to next cell.
NSGA-II can then be used to find a set of Pareto optimal solutions in \cite{Ahmed2013}.
The solutions would be obtained as sequences of way points.
To better support the environment with obstacles, spline are introduced to interpolate a sequence of way points into a trajectory in \cite{6181426}.
It reveals the drawbacks of this format of path trajectory. 
This process might also deviate the path from the found optimal result.
Increasing the path resolution, the number of the steps, should better deal with the obstacles in the working space and also follows better along the gradient of the fitness.
But the dimension of the solution space would be increased.
As a result, the search time could be significantly expanded.
The exist of obstacles also make plenty of paths infeasible.
This leads to the discontinuity of the solution space, which could impact the performance of many evolutionary algorithm on multi-objective optimization.

The discretized map only approximates the environment, which might lose a lot of environmental information.
Some of the obstacles' shapes could not be well considered in the planning process.

If we model the problem in a continuous space, the solution space is greatly expanded. 
It will take much longer time to converge to the Pareto front.
RRT(Rapidly exploring random tree) has been a popular tool to path planning in a continuous space.
It efficiently explores the space by randomly sampling and works well with any complex obstacles.
The simplicity of the algorithm ensures the search time of a feasible solution can be very short.
It is figured out that RRT fails to find an optimal solution in the optimal path planning in \cite{Karaman.Frazzoli:RSS10}.
RRT* is introduced to facilitate the optimal search.
A \emph{Rewire} process is imported to update the tree structure towards optimality.
It has been proven that RRT* shows asymptotic optimality.
It actually learns the optimal path from each vertex of the tree structure to the start position.
Figure \ref{fig:RRTstar2} gives an example.
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{fig/RRTstar2}
\caption{Tree structure after exploring fitness space.}
\label{fig:RRTstar2}
\end{figure}

Besides dominance comparison, decomposition is another effective way to find Pareto optimality.
MOEA-D is proposed in \cite{4358754}, which decomposes a multi-objective optimization problem into a class of subproblems.
Three types of decomposition methods are introduced in \cite{4358754}.
\begin{itemize}
\item 
\item 
\item 
\end{itemize}
A few evolutionary algorithms based on the decomposition are proposed to solve the multi-objective optimization~\cite{6600851}, 
especially in many-objective optimization problems.
It is noticeable that the decomposition depends on finding the ``Utopia'' point as a reference to minimize different weighted distances.
In the next section, we will propose an algorithm that explores the solution spaces parallel using an RRT* tree structure.



\section{Multi-Objective Rapidly exploring Random Forest$^{*}$}
\label{sec:morrt}

The multi-objective path planning problem is firstly defined.
Given a multi-objective optimization problem with $ K $ objectives,
there are $ K $ objectives to be minimized. 
The solution set will be obtained by decomposing it into $ N $ subproblems.

There are two types of tree structures used for the optimization process.
\begin{itemize}
\item Each \emph{reference tree} explores for a single objective $ f_{k} (x), k \in K $. 
\item Each \emph{subproblem tree} explores for a subproblem $ g_{n} ( x \mid \lambda_{n} , z^{*} ) , n \in N $.
\end{itemize}
The paths found from both the subproblem trees and the reference trees consist of the solution set.

Because the convergence of the tree structure 


Like all the sampling-based optimization, the random positions are uniformly sampled from the workspace.
It means that all the tree have equivalent vertices constructed from same positions set sequentially.
But they are connected by different measurements of the costs, either a single objective or a cost from subproblem definition.
The edges of the trees can be different.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{./fig/MORRTstar}
\caption{Rapidly Exploring Process}
\label{fig:MORRTstar}
\end{figure}


\begin{algorithm}
\begin{algorithmic}[1]
\For{ \textbf{each} $ V_{r} \in \mathbf{V}_{r} $ } 
	\State $ V_{r} \leftarrow \{ x_{init} \} $; $ E_{r} \leftarrow \emptyset $; $ i \leftarrow 0 $
\EndFor
\For{ \textbf{each} $ V_{s} \in \mathbf{V}_{s} $ } 
	\State $ V_{s} \leftarrow \{ x_{init} \} $; $ E_{s} \leftarrow \emptyset $; $ i \leftarrow 0 $
\EndFor
\While{ $ i < N $ }
	\For{ \textbf{each} $ G_{r} \in \mathbf{G}_{r} $ } 
		\State $ G_{r} \leftarrow (V_{r}, E_{r}) $
	\EndFor
	\For{ \textbf{each} $ G_{s} \in \mathbf{G}_{s} $ } 
		\State $ G_{s} \leftarrow (V_{s}, E_{s}) $
	\EndFor
	\State $ x_{rand} \leftarrow $ \Call{ Sample }{$ i $} ; $ i \leftarrow i + 1 $
	\State $ V' \leftarrow V $; $ E' \leftarrow E $
	\State $ x_{nearest} \leftarrow $ \Call{Nearest}{$ G, x $}
	\State $ x_{new} \leftarrow $ \Call{Steer}{$ x_{nearest}, x $}
	\If{ \Call{ObstacleFree}{$ x_{nearest}, x_{new} $} }
		\For{ \textbf{each} $ G_{r} \in \mathbf{G}_{r} $ } 
		\State $ (V_{r}, E_{r}) \leftarrow $ \Call{ Extend$_{Ref}$ }{$ G_{r}, x_{new} $}
		\EndFor
		\For{ \textbf{each} $ G_{s} \in \mathbf{G}_{s} $ } 
		\State $ (V_{s}, E_{s}) \leftarrow $ \Call{ Extend$_{Sub}$ }{$ G_{s}, x_{new} $}
		\EndFor
	\EndIf
\EndWhile
\end{algorithmic}
\label{alg:rapidly_exploring_process}
\caption{Multi-objective Rapidly Random exploring }
\end{algorithm}

\begin{algorithm}
\begin{algorithmic}[1]
\State $ V' \leftarrow V' \cup \{ x_{new} \} $
\State $ x_{min} \leftarrow x_{nearest} $
\State $ X_{near} \leftarrow $ \Call{Near}{$ G, x_{new}, | V | $}
\For{\textbf{each} $ x_{near} \in X_{near} $ }
	\If{ \Call{ObstacleFree}{$ x_{new} , x_{near} $} }
		\State $ c_{k}' \leftarrow $ \Call{Cost$_{k}$}{$ x_{near} $} $ + c_{k}( $ \Call{Line}{$ x_{near}, x_{new} $} $ ) $ 
		\If{ $ c_{k}' < $ \Call{Cost$_{k}$}{$ x_{new} $} }
		\State $ x_{min} \leftarrow x_{near} $
		\EndIf
	\EndIf
\EndFor
\State $ E' \leftarrow E' \cup \{ ( x_{min}, x_{new} ) \} $
\For{\textbf{each} $ x_{near} \in X_{near} \setminus \{ x_{min} \} $ }
	\If{\Call{ObstacleFree}{$ x_{new} , x_{near} $}}
	    \State $ c_{k}' \leftarrow $ \Call{Cost$_{k}$}{$ x_{new} $} $ + c_{k}( $ \Call{Line}{$ x_{new}, x_{near} $} $ ) $ 
	    \If{ $ c_{k}' < $ \Call{Cost$_{k}$}{$ x_{near} $} }
			\State $ x_{parent} \leftarrow $ \Call{Parent}{$ x_{near} $}
			\State $ E' \leftarrow E' \setminus \{ ( x_{parent}, x_{near} ) \} $
			\State $ E' \leftarrow E' \cup \{ ( x_{new}, x_{near} ) \} $
		\EndIf
	\EndIf
\EndFor
\Return $ G' = (V', E') $ 
\end{algorithmic}
\label{alg:morrtstar:extend:ref}
\caption{ $ \mbox{Extend}_{Ref} (G, x) $}
\end{algorithm} 

\begin{algorithm}
\begin{algorithmic}[1]
\State $ V' \leftarrow V' \cup \{ x_{new} \} $
\State $ x_{min} \leftarrow x_{nearest} $
\State $ X_{near} \leftarrow $ \Call{Near}{$ G, x_{new}, | V | $}
\For{\textbf{each} $ x_{near} \in X_{near} $ }
	\If{ \Call{ObstacleFree}{$ x_{new} , x_{near} $} }
		\State $ \vec{c}' \leftarrow $ \Call{Cost}{$ x_{near} $} $ + \vec{c}( $ \Call{Line}{$ x_{near}, x_{new} $} $ ) $ 
		\State $ \eta' =  $ \Call{Fitness}{ $ \vec{c}' , \vec{c}^{*} \mid \lambda_{G} $ }
		\State $ \vec{c}_{new} = $ \Call{Cost}{$ x_{new} $} 
		\State $ \eta_{new} = $ \Call{Fitness}{ $ \vec{c}_{new} , \vec{c}^{*} \mid \lambda_{G} $ }
		\If{ $ \eta' < \eta_{new} $ }
			\State $ x_{min} \leftarrow x_{near} $
		\EndIf
	\EndIf
\EndFor
\State $ E' \leftarrow E' \cup \{ ( x_{min}, x_{new} ) \} $
\For{\textbf{each} $ x_{near} \in X_{near} \setminus \{ x_{min} \} $ }
	\If{\Call{ObstacleFree}{$ x_{new} , x_{near} $} }
		\State $ \vec{c}' \leftarrow $ \Call{Cost}{$ x_{new} $} $ + \vec{c}( $ \Call{Line}{$ x_{new}, x_{near} $} $ ) $ 
		\State $ \eta' =  $ \Call{Fitness}{ $ \vec{c}' , \vec{c}^{*} \mid \lambda_{G} $ }
		\State $ \vec{c}_{near} = $ \Call{Cost}{$ x_{near} $} 
		\State $ \eta_{near} = $ \Call{Fitness}{ $ \vec{c}_{near} , \vec{c}^{*} \mid \lambda_{G} $ }
		\If{ $ \eta' < \eta_{near} $ }
			\State $ x_{parent} \leftarrow $ \Call{Parent}{$ x_{near} $}
			\State $ E' \leftarrow E' \setminus \{ ( x_{parent}, x_{near} ) \} $
			\State $ E' \leftarrow E' \cup \{ ( x_{new}, x_{near} ) \} $
		\EndIf
	\EndIf
\EndFor
\Return $ G' = (V', E') $ 
\end{algorithmic}
\label{alg:morrtstar:extend:sub}
\caption{ $ \mbox{Extend}_{Sub} (G, x) $}
\end{algorithm} 


\section{Theoretic Analysis}
\label{sec:theoretic_analysis}

\begin{thm}
	\label{thm:moo-d:rrt}
	The decomposition method is applicable to a RRT structure.
	\begin{proof}
		Let the path from start to goal with the most vertices be $ N $ segments.
		Any other paths can be represented into $ N $ segments (because they have less than $ N $ segment).
		By this way, all the paths could be mapped into a $ N $ dimensional space.
	\end{proof}
\end{thm}

\begin{lem}
\label{lem:ref_tree:conv}
The reference tree will converge.
\begin{proof}
Haha
\end{proof}
\end{lem}

\begin{lem}
\label{lem:sub_tree:conv}
The subproblem tree will converge.
\begin{proof}
Baba
\end{proof}
\end{lem}

\begin{thm}
\label{thm:morrt:asym_opt}
\end{thm}

\section{Simulation}
\label{sec:simulation}


\section{Conclusion} 
\label{sec:conclusion}

The conclusion goes here.

\section*{Acknowledgments}

%% Use plainnat to work nicely with natbib. 

\bibliographystyle{plainnat}
\bibliography{reference}

\end{document}